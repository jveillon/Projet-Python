{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d52f9621-910c-4c05-bb30-465202e68858",
   "metadata": {},
   "source": [
    "Après avoir sélectionné la méthode de clustering la plus efficace, nous allons analyser le vocabulaire utilisé dans les tweets de chaque cluster.\n",
    "Avant toute chose, il est nécessaire de nettoyer les tweets, c'est-à-dire de supprimer la ponctuation, les majuscules, les caractères spéciaux, les emojis, etc.\n",
    "On va également créer une liste de stopwords à supprimer.\n",
    "La fonction suivante prend en argument un texte et renvoie le texte nettoyé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "783ad3ea-9e26-44ac-85d3-54d4123044bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting demoji\n",
      "  Downloading demoji-1.1.0-py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: demoji\n",
      "Successfully installed demoji-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install demoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fea95dd-a883-4415-9b44-d1def3a0b09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.8.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (458 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m458.9/458.9 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.6.1 in /opt/mamba/lib/python3.10/site-packages (from wordcloud) (1.23.5)\n",
      "Requirement already satisfied: pillow in /opt/mamba/lib/python3.10/site-packages (from wordcloud) (9.2.0)\n",
      "Requirement already satisfied: matplotlib in /opt/mamba/lib/python3.10/site-packages (from wordcloud) (3.6.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/mamba/lib/python3.10/site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/mamba/lib/python3.10/site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/mamba/lib/python3.10/site-packages (from matplotlib->wordcloud) (4.38.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/mamba/lib/python3.10/site-packages (from matplotlib->wordcloud) (1.0.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/mamba/lib/python3.10/site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/mamba/lib/python3.10/site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/mamba/lib/python3.10/site-packages (from matplotlib->wordcloud) (21.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/mamba/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.8.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a91bf9a-91ab-4719-82fb-d92881192051",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import string\n",
    "import demoji\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import wordcloud\n",
    "import numpy as np\n",
    "import io\n",
    "import requests\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb7be54-af26-432a-8850-c6fe7fe77ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"tweets.json\", orient = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea42d9b-1d3b-4238-80b0-3da7836c310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rajouter les mots qui apparaissent très souvent et qui ne sont pas utiles pour l'analyse\n",
    "stop_words_context = [\"france\", \"pologne\", \"but\"]\n",
    "stop_words = set(stopwords.words('french'))\n",
    "stop_words = list(stop_words) + stop_words_context\n",
    "\n",
    "def rm_stopwords(text):\n",
    "    return [w for w in text.split() if w not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8134250e-053c-468d-9597-c5b43fc7d6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    #tout mettre en minuscules\n",
    "    text = text.lower()\n",
    "    #suppression de la ponctuation\n",
    "    punct = string.punctuation\n",
    "    text = text.translate(str.maketrans(\", \", punct))\n",
    "    #suppression des chiffres (pour une analyse de vocabulaire, ils ne sont pas nécessaires)\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "    #suppression des emojis\n",
    "    for item in demoji.findall(text):\n",
    "        text =text.replace(item,\"\")\n",
    "    #suppression des mentions\n",
    "    text = re.sub(\"@[A-Za-z0-9_]+\",\"\", text)\n",
    "    #suppression des hashtags\n",
    "    text = re.sub(\"#[A-Za-z0-9_]+\",\"\", text)\n",
    "    #suppression des stopwords\n",
    "    text = rm_stopwords(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a307891-dd10-47f2-9391-ecce08ee3d97",
   "metadata": {},
   "source": [
    "On va ensuite créer un wordcloud pour chaque cluster.\n",
    "Première étape : dans le dataframe des tweets, créer une nouvelle variable \"clean text\" qui contient le texte des tweets modifiés grâce à la fonction clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71030dc-da69-42fa-b727-4c4d33a90103",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clean_text\"] = df[\"text\"].apply(lambda x: clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa35d79-ea6d-45e3-9d80-0d629e955a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "def make_wordcloud(corpus):\n",
    "    wc = wordcloud.WordCloud(background_color=\"white\", max_words=2000)\n",
    "    wc.generate(corpus)\n",
    "    return wc\n",
    "\n",
    "#j'ai repris le code du TP, il faudra faire une boucle pour appliquer ça à chaque cluster\n",
    "plt.imshow(make_wordcloud(dumas), interpolation='bilinear')\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce5f3ef-c507-47f7-8ab5-9f8b35d3052d",
   "metadata": {},
   "source": [
    "On va maintenant créer différentes variables concernant les tweets, de manière à analyser d'autres caractéristiques que le vocabulaire.\n",
    "Pour notre analyse, il peut être utile de connaître le nombre de hashtags dans chaque tweet, le nombre de majuscules ainsi que le nombre de points d'exclamation.\n",
    "Le dataframe initial contient déjà le nombre de likes et de retweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b24580-ffe4-4b2a-9c4e-6fa3f6447f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_hashtags(text):\n",
    "    find = re.compile(\"([#]\\w+)\").findall(text)\n",
    "    return len(find)\n",
    "\n",
    "df[\"nb_hashtags\"] = df[\"text\"].apply(lambda x : count_hashtags(x))\n",
    "\n",
    "def count_exclamation(text):\n",
    "    find = re.compile(\"(\\w?\\s?[!])\").findall(text)\n",
    "    return len(find)\n",
    "\n",
    "df[\"nb_exclamation\"] = df[\"text\"].apply(lambda x : count_exclamation(x))\n",
    "\n",
    "def count_maj(text):\n",
    "    find = re.compile(\"([A-Z][A-Z]+)\").findall(text)\n",
    "    return len(find)\n",
    "\n",
    "df[\"nb_maj\"] = df[\"text\"].apply(lambda x : count_maj(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b868e438-b658-4a0a-8dcd-171ef31c2e33",
   "metadata": {},
   "source": [
    "Statistiques descriptives sur les variables, par cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aa3118-3758-4af5-bdd5-7df0afb78c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(df.describe(), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5a92eb-2897-4ee9-b511-d8fe0661b378",
   "metadata": {},
   "source": [
    "Il faut ajouter une variable \"cluster\" dans le dataframe, qui contient l'identifiant du cluster auquel appartient chaque utilisateur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a986c4-a6fd-43a6-9966-0199769af45c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
